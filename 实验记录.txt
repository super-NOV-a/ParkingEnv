从CNN开始训练，其网络使用的是环形卷积+MLP

从ppo_arc_cnn开始训练，在空白场地上实现停车，且任务成功的判定条件为 距离小于2 角度小于30度
cnn1使用的是：停车率还不错，不过存在一些情况死活停不进去
	world_size=30.0,
        occupy_prob=0.2,          # 初级课程
        gap=4.0,
cnn2使用更大的场景，更多障碍：
	world_size=40.0,
        occupy_prob=0.3,          # 初级课程
        gap=4.0,


empty的cnn和fc都是：
log1 	使用旧的课程 旧的观测 72+5
log2  使用新的观测 72+7，角度的三角函数和重新归一化的距离，此外最开始的课程变得更简单了，很容易到达


后续的课程设计没太大变化，目前只有max_len修改了

random的cnn
log1 从零开始学习，学习缓慢
log2 导入empty的log2继续训练，看起来没有学到有效信息，感觉可以从random_cnn log1继续学  或者从较大的场景开始学

现在已经没办法使用cnn log1，因为其已经被log2覆盖了，
random cnn1  设置其环境大小为40m，训练步数放大到4×现在的长度
--!--观测中增加last动作，观测 72+9 （雷达72+自车状态（速度、转角）2+目标距离1+目标相对方位2+目标相对朝向2+上一状态（速度、转角）2）
random cnn2 观测增加上一步动作，看起来训练缓慢

random fc1 观测增加上一步动作，看起来和cnn1类似，到达难度为10后有点退步，是不是因为难度修改太快，将修改难度调整为1000个回合后再训练fc2
random fc2 观测增加上一步动作，每1000个回合修改难度
random fc3 动作空间中的角度修改为[-30, -24, -18, -12, -8, -5, -2, 0, 2, 5, 8, 12, 18, 24, 30] 非均匀的转角动作分布 ，且修改难度配置
现在的难度会修改gap从4.5到0.5和occupy_prob从0.05到0.95,（也有原本的停车成功判定难度）难度可能较高
--fc3训练了两次，第一次无法应对存在障碍的情况，可能是环境障碍太多，倾向于避开，不过在难度为10的random上测试结果不错，存在一些车位没办法到达的情况，这是我的问题！--todo车位之间给出合适的距离

random fc4 将难度梯度降低接近一半，先在简单环境训练一个基础模型，叫做Train_easy，是fc1和fc2的水平，准备再看看效果
根据难度对奖励进行修正，鼓励智能体在高难度中完成任务

file fc4 使用json的真实感知环境，其成功率提升困难？但是具体原因没找到，佳睿在该环境使用几何中心（车位重叠的障碍删去的情况下效果不错）
box fc4 使用box环境，更纯粹的停车吧，在random中有平行停车理论上没办法听进去的问题----todo
box fc5 使用修改后的box，目标只会出现在中心位置，随着难度提高，车位3面障碍概率直线提升，在这种情况下进行训练

下面几组box雷达长度为30
box fc6 生成时限制距离在8m到world边缘，初始距离与难度无关，难度只修改目标进去的难度
box fc7 ----继续训练的fc6，在box难度10上成功率接近90，不过可能过拟合了，泛化性差，在random和file上表现很差，file成功率约20%---box上效果很好
random_box fc6  使用random_box,两种场景均存在的情况下，从fc6最终模型 难度6继续训练
random_box fc7  从fc6 难度9继续，其结果在box和random上平均成功率接近80，不过泛化性依然堪忧，考虑更多样的随机场景（四种都有）

box fc8   使用雷达减去阈值作为观测，对比一下与fc6的结果， 发现这种情况训练更差了！！！  与直觉不符
random_box fc8 ，这个场景中存在若干不同的场景 random box file empty都有。从难度0从头训练。后续又在此基础上进行训练，但是成功率保持在65上下不再升高，难度在95左右
random_box cnn0  经短时间训练，看到cnn的收敛速度远不如fc，因此暂时不考虑cnn

file fc0_1在 随机初始位置的情况下，训练效果较差？（此处存在转角变化约束，前后action转角差不大于30度）
file fc0_energy 使用energy，并且去掉初始的ego随机位置--这个是cnn
file fc0_no_energy 不使用energy，作为消融实验，证明energy效果--确实有效的--这个也是cnn

random_box cnn1  使用random box empty三种情况进行训练，与cnn0设置基本相同，不过收敛看起来堪忧啊
incremental random_box cnn0  使用连续增量动作训练，收敛更慢，且步数较长
-----------------------------之前的模型不能再使用了，观测空间发生了变化
incremental random_box fc0   前两次朝向修改不惩罚、增加朝向和改变次数观测  修改雷达为空范围  动作空间是连续空间--！！promising！！--
--连续动作像个傻子
arc random_box fc0	限制每次转角增量30度以内  训练情况还是很好的，他10M步基本上保证能到level9-level10之间
----离散弧长动作空间：{离散偏转角}{-1, -0.25, 0.25, 1}
离散加速度动作空间：{离散偏转角}{-1,0,1}

使用--arc random_box fc0--10M步作为初始模型，然后在parking上继续训练，
第一个parking是左右两停车区域，中间是行车道，第二个parking是避障前进+泊车
arc parking fc0 第一个parking环境，模拟停车场，world_size变为25，radar长度为15，可能过于简单，泛化性会变差
设计parking环境，希望车辆学习到 -- 1.避开碰撞障碍、2.移动到车位附近、3.调整自身位置
arc parking fc00  重改parking环境，前进避障后停车，也是使用课程学习，到达目标的难度放宽了--00开始的效果还不错，基本达到预期
arc parking fc01  略微修改奖励后开始01 从fc00中断的位置继续训练的
arc parking fc02  略微修改parking难度调节：障碍数量、位置、泊车空间，从fc01中途开始训练
fc00及以后的模型使用的场景是长parking level0和level10是一个长度，但是easy是从短到长的
修改parking奖励设计，修改模型网络结构，现在可以选择mlp、cnn、rnn总共四种网络，暂时还没有实验，理论上现在的代码支持四种结构
arc parking fc_easy  想先训练一个能自己走到目标前并且调整车头，倒车入库的模型。-目前4M步后刚到达level 10难度就降下来了

fc_easy_rnn_1  	设置n_steps为32，训练收敛缓慢，且看运行fps低于170
fc_easy_rnn	设置n_steps为2048，与mlp保持一致，看它在相同步数内的效果


----接下来看一下Q-chunking

限制RL策略更新时靠近行为策略，不过此处的行为策略是从Data中用flow-matching训练的行为策略（flow policy）
从行为策略中采样N个中最大Q值的动作
限制转向增量应该能实现一个比较合理的离散动作分解--todo
====此处需要反驳一下做动作分块的理由，离散弧长已经是一个连续轨迹，没必要从离散弧长分解再做动作分块了

--课程学习的调整内容--
empty：
课程会调整：到达目标判定条件、初始目标距离
random：
课程会调整：到达目标判定条件、障碍车辆密度
box：
课程调整：到达目标判定条件、障碍box距离车位的距离
file：
课程调整：到达目标判定条件
